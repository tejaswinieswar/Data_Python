{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: collecting and storing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,re,requests\n",
    "from bs4 import BeautifulSoup # importing beautiful soup to extract data from html files\n",
    "path_to_data='/Users/Sneha/Documents/python/Python_assignments/midterm/data' #  relative path where data will get stored\n",
    "os.mkdir(path_to_data,777);\n",
    "os.chmod(path_to_data,int('0755'))\n",
    "api_key_arch='17822675e69c4b5e9161ee00b477e290' # api key for archive api method\n",
    "most_popular_arts='e7a1e81cd64649679ebb08af6776bd72' # api key for most popular api method\n",
    "\n",
    "#definition to collect and store archive_data\n",
    "def archive_data():\n",
    "    os.mkdir(path_to_data+'/Archive_data', 777);\n",
    "    os.chmod(path_to_data +'/Archive_data', int('0755'));\n",
    "    # Gather data from the year 2005 to 2016\n",
    "    for year in range(2005,2017):\n",
    "        os.mkdir(path_to_data + '/Archive_data'+'/'+str(year), 777);\n",
    "        os.chmod(path_to_data + '/Archive_data'+'/'+str(year), int('0755'));\n",
    "        current_path= path_to_data + '/Archive_data'+'/'+str(year)\n",
    "\n",
    "        # Gather data for month of January\n",
    "        for month in range(1,2):\n",
    "            proper_data = ''\n",
    "            # URL query for archive method with year and month\n",
    "            url_arc = 'https://api.nytimes.com/svc/archive/v1/' + str(year) + '/' + str(month) + '.json?&api-key=' + api_key_arch\n",
    "            # getting html using requests module\n",
    "            r = requests.get(url_arc)\n",
    "            data = str(r.json())\n",
    "            url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', data)\n",
    "            for every in url:\n",
    "                # Traversing through only new york times website and with options data\n",
    "                f = open(current_path + '/' + str(month) + '.txt', 'a')\n",
    "                if every.startswith('http://www.nytimes.com/') and '/opinion/' in every:\n",
    "                    print(every)\n",
    "                    # Request data fpr each url\n",
    "                    data = requests.get(every[:-2])\n",
    "                    d = data.text\n",
    "                    Bsoup = BeautifulSoup(data.content, \"html.parser\")\n",
    "                    # Find only the data from the html text\n",
    "                    h = Bsoup.find_all('p', {\"class\": \"story-body-text story-content\"})\n",
    "                    for link in h:\n",
    "                        li = str(link)\n",
    "                        # Fetch data after the 'p' tags\n",
    "                        spli_val = li[:-3].split('<')\n",
    "                        for loop in spli_val:\n",
    "                            if '>' in loop:\n",
    "                                dats = loop.split('>')\n",
    "                                #Encode data into utf-8 format\n",
    "                                dataa=dats[1].encode('utf-8')\n",
    "                                dt=str(dataa)\n",
    "                                f.write(dt)\n",
    "\n",
    "            f.close()\n",
    "\n",
    "#Definition for most popular api method\n",
    "def most_popular():\n",
    "    #Types in the most popular api method\n",
    "    types=['Arts','Books','Food','Health','Magazine','Science','Sports','Style','Travel','World']\n",
    "    for type in types:\n",
    "        os.mkdir(path_to_data + '/Most_Popular/'+type, 777);\n",
    "        os.chmod(path_to_data + '/Most_Popular/'+type, int('0755'));\n",
    "        # URL query for most popular method with start and end date\n",
    "        url_pop = 'https://api.nytimes.com/svc/mostpopular/v2/mostshared/'+type+'/30'+ '.json?&api-key=' + most_popular_arts\n",
    "        r = requests.get(url_pop)\n",
    "        data = str(r.json())\n",
    "        # Find all URL's\n",
    "        url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', data)\n",
    "\n",
    "        for every in url:\n",
    "            f = open(path_to_data + '/Most_Popular/'+type + '/' + 'Data.txt', 'a')\n",
    "            print(every)\n",
    "            # Request data fpr each url\n",
    "            data=requests.get(every[:-2])\n",
    "            print (data)\n",
    "            d=data.text\n",
    "            print (d)\n",
    "            Bsoup = BeautifulSoup(data.content, \"html.parser\")\n",
    "            # Find only the data from the html text\n",
    "            h=Bsoup.find_all('p', {\"class\" : \"story-body-text story-content\"})\n",
    "            for link in h:\n",
    "                li= str(link)\n",
    "                # Fetch data after the 'p' tags\n",
    "                spli_val=li[:-3].split('<')\n",
    "                for loop in spli_val:\n",
    "                    if '>' in loop:\n",
    "                        dats=loop.split('>')\n",
    "                        dataa=dats[1].encode('utf-8')\n",
    "                        dt=str(dataa)\n",
    "                        f.write(dt)\n",
    "            f.close()\n",
    "archive_data()\n",
    "most_popular()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
